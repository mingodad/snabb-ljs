// Use of this source code is governed by the Apache 2.0 license; see COPYING.

// Implements virtio-net device


module(...,package.seeall);

var lib       = require("core.lib");
var link      = require("core.link");
var memory    = require("core.memory");
var packet    = require("core.packet");
var timer     = require("core.timer");
var VirtioVirtq = require("lib.virtio.virtq_device");
var checksum  = require("lib.checksum");
var ffi       = require("ffi");
var C         = ffi.C;
var band      = bit.band;

require("lib.virtio.virtio.h");
require("lib.virtio.virtio_vring_h");

var char_ptr_t = ffi.typeof("char *");
var virtio_net_hdr_size = ffi.sizeof("struct virtio_net_hdr");
var virtio_net_hdr_type = ffi.typeof("struct virtio_net_hdr *");
var virtio_net_hdr_mrg_rxbuf_size = ffi.sizeof("struct virtio_net_hdr_mrg_rxbuf");
var virtio_net_hdr_mrg_rxbuf_type = ffi.typeof("struct virtio_net_hdr_mrg_rxbuf *");

var invalid_header_id = 0xffff;

/*
   A list of what needs to be implemented in order to fully support
   some of the options.

   - VIRTIO_NET_F_CSUM - enables the SG I/O (resulting in
      multiple chained data buffers in our TX path(self.rxring))
      Required by GSO/TSO/USO. Requires CSUM offload support in the
      HW driver (now intel_mp)

   - VIRTIO_NET_F_MRG_RXBUF - enables multiple chained buffers in our RX path
      (self.txring). Also chnages the virtio_net_hdr to virtio_net_hdr_mrg_rxbuf

   - VIRTIO_F_ANY_LAYOUT - the virtio_net_hdr/virtio_net_hdr_mrg_rxbuf is "prepended"
      in the first data buffer instead if provided by a separate descriptor.
      Supported in fairly recent (3.13) Linux kernels

   - VIRTIO_RING_F_INDIRECT_DESC - support indirect buffer descriptors.

   - VIRTIO_NET_F_CTRL_VQ - creates a separate control virt queue

   - VIRTIO_NET_F_MQ - multiple RX/TX queues, usefull for SMP (host/guest).
      Requires VIRTIO_NET_F_CTRL_VQ

--*/
var supported_features = C.VIRTIO_F_ANY_LAYOUT +
                           C.VIRTIO_NET_F_CTRL_VQ +
                           C.VIRTIO_NET_F_MQ +
                           C.VIRTIO_NET_F_CSUM +
                           C.VHOST_USER_F_PROTOCOL_FEATURES;
/*
   The following offloading flags are also available:
   VIRTIO_NET_F_CSUM
   VIRTIO_NET_F_GUEST_CSUM
   VIRTIO_NET_F_GUEST_TSO4 + VIRTIO_NET_F_GUEST_TSO6 + VIRTIO_NET_F_GUEST_ECN + VIRTIO_NET_F_GUEST_UFO
   VIRTIO_NET_F_HOST_TSO4 + VIRTIO_NET_F_HOST_TSO6 + VIRTIO_NET_F_HOST_ECN + VIRTIO_NET_F_HOST_UFO
*///

var max_virtq_pairs = 16;

VirtioNetDevice = {};

function VirtioNetDevice::new(owner, disable_mrg_rxbuf, disable_indirect_desc) {
   assert(owner);
   var o = {
      owner = owner,
      callfd = {},
      kickfd = {},
      virtq = {},
      rx = {},
      tx = {
         p = null,
         tx_mrg_hdr = ffi.new("struct virtio_net_hdr_mrg_rxbuf*[1]") ,
         data_sent = null,
         finished = null
      }
   };

   o = setmetatable(o, {__index = VirtioNetDevice});

   for( i = 0, max_virtq_pairs-1 ) {
      // TXQ
      o.virtq[2*i] = VirtioVirtq->new();
      o.virtq[2*i].device = o;
      // RXQ
      o.virtq[2*i+1] = VirtioVirtq->new();
      o.virtq[2*i+1].device = o;
   }

   this.virtq_pairs = 1;
   this.hdr_type = virtio_net_hdr_type;
   this.hdr_size = virtio_net_hdr_size;

   this.supported_features = supported_features;

   if( ! disable_mrg_rxbuf ) {
      this.supported_features = this.supported_features
         + C.VIRTIO_NET_F_MRG_RXBUF;
   }
   if( ! disable_indirect_desc ) {
      this.supported_features = this.supported_features
         + C.VIRTIO_RING_F_INDIRECT_DESC;
   }

   return o;
}

function VirtioNetDevice::poll_vring_receive () {
   // RX
   this->receive_packets_from_vm();
   this->rx_signal_used();
}

// Receive all available packets from the virtual machine.
function VirtioNetDevice::receive_packets_from_vm () {
   if( this.receive_packets_ops == null ) {
      this.receive_packets_ops = {
         packet_start = this.rx_packet_start,
         buffer_add   = this.rx_buffer_add,
         packet_end   = this.rx_packet_end
      };
   }
   for( i = 0, this.virtq_pairs-1 ) {
      this.ring_id = 2*i+1;
      var virtq = this.virtq[this.ring_id];
      virtq->get_buffers('rx', this.receive_packets_ops, this.hdr_size);
   }
}

function VirtioNetDevice::rx_packet_start(addr, len) {
   var rx_p = packet.allocate();

   var rx_hdr = ffi.cast(virtio_net_hdr_type, this->map_from_guest(addr));
   this.rx_hdr_flags = rx_hdr.flags;
   this.rx_hdr_csum_start = rx_hdr.csum_start;
   this.rx_hdr_csum_offset = rx_hdr.csum_offset;

   return rx_p;
}

function VirtioNetDevice::rx_buffer_add(rx_p, addr, len) {

   addr = this->map_from_guest(addr);
   var pointer = ffi.cast(char_ptr_t, addr);

   packet.append(rx_p, pointer, len);
   return len;
}

function VirtioNetDevice::rx_packet_end(header_id, total_size, rx_p) {
   var l = this.owner.output.tx;
   if( l ) {
      if( band(this.rx_hdr_flags, C.VIO_NET_HDR_F_NEEDS_CSUM) != 0 &&
         // Bounds-check the checksum area
         this.rx_hdr_csum_start  <= rx_p.length - 2 &&
         this.rx_hdr_csum_offset <= rx_p.length - 2
      ) {
         checksum.finish_packet(
            rx_p.data + this.rx_hdr_csum_start,
            rx_p.length - this.rx_hdr_csum_start,
            this.rx_hdr_csum_offset);
      }
      link.transmit(l, rx_p);
   } else {
      debug("droprx", "len", rx_p.length);
      packet.free(rx_p);
   }
   this.virtq[this.ring_id]->put_buffer(header_id, total_size);
}

// Advance the rx used ring and signal up
function VirtioNetDevice::rx_signal_used() {
   for( i = 0, this.virtq_pairs-1 ) {
      this.virtq[2*i+1]->signal_used();
   }
}

function VirtioNetDevice::poll_vring_transmit () {
   // RX
   this->transmit_packets_to_vm();
   this->tx_signal_used();
}

// Receive all available packets from the virtual machine.
function VirtioNetDevice::transmit_packets_to_vm () {
   if( this.transmit_packet_ops == null ) {
      if( ! this.mrg_rxbuf ) {
         this.transmit_packet_ops = {
            packet_start = this.tx_packet_start,
            buffer_add   = this.tx_buffer_add,
            packet_end   = this.tx_packet_end
         };
      } else {
         this.transmit_packet_ops = {
            packet_start = this.tx_packet_start_mrg_rxbuf,
            buffer_add   = this.tx_buffer_add_mrg_rxbuf,
            packet_end   = this.tx_packet_end_mrg_rxbuf
         };
      }
   }
   for( i = 0, this.virtq_pairs-1 ) {
      this.ring_id = 2*i;
      var virtq = this.virtq[this.ring_id];
      virtq->get_buffers('tx', this.transmit_packet_ops, this.hdr_size);
   }
}

var function validflags(buf, len) {
   var valid = checksum.verify_packet(buf, len);

   if( valid == true ) {
      return C.VIO_NET_HDR_F_DATA_VALID;
   } else if( valid == false ) {
      return 0;
   } else {
      return C.VIO_NET_HDR_F_NEEDS_CSUM;
   }
}




function VirtioNetDevice::tx_packet_start(addr, len) {
   var l = this.owner.input.rx;
   assert(l, "input port not found");
   if( link.empty(l) ) { return null, null; }
   var tx_p = link.receive(l);

   var tx_hdr = ffi.cast(virtio_net_hdr_type, this->map_from_guest(addr));

   // TODO: copy the relevnat fields from the packet
   ffi.fill(tx_hdr, virtio_net_hdr_size);
   if( band(this.features, C.VIRTIO_NET_F_CSUM) == 0 ) {
      tx_hdr.flags = 0;
   } else {
      assert(tx_p.length > 14);
      tx_hdr.flags = validflags(tx_p.data+14, tx_p.length-14);
   }

   return tx_p;
}

function VirtioNetDevice::tx_buffer_add(tx_p, addr, len) {

   addr = this->map_from_guest(addr);
   var pointer = ffi.cast(char_ptr_t, addr);

   assert(tx_p.length <= len);
   ffi.copy(pointer, tx_p.data, tx_p.length);

   return tx_p.length;
}

function VirtioNetDevice::tx_packet_end(header_id, total_size, tx_p) {
   packet.free(tx_p);
   this.virtq[this.ring_id]->put_buffer(header_id, total_size);
}

function VirtioNetDevice::tx_packet_start_mrg_rxbuf(addr, len) {
   var tx_mrg_hdr = ffi.cast(virtio_net_hdr_mrg_rxbuf_type, this->map_from_guest(addr));
   var l = this.owner.input.rx;
   assert(l, "input port not found");
   var tx_p = this.tx.p;
   ffi.fill(tx_mrg_hdr, virtio_net_hdr_mrg_rxbuf_size);

   // for the first buffer receive a packet and save its header pointer
   if( ! tx_p ) {
      if( link.empty(l) ) { return; }
      tx_p = link.receive(l);

      if( band(this.features, C.VIRTIO_NET_F_CSUM) == 0 ) {
         tx_mrg_hdr.hdr.flags = 0;
      } else {
         tx_mrg_hdr.hdr.flags = validflags(tx_p.data+14, tx_p.length-14);
      }

      this.tx.tx_mrg_hdr[0] = tx_mrg_hdr;
      this.tx.data_sent = 0;
   }

   return tx_p;
}

function VirtioNetDevice::tx_buffer_add_mrg_rxbuf(tx_p, addr, len) {

   addr = this->map_from_guest(addr);
   var pointer = ffi.cast(char_ptr_t, addr);

   // The first buffer is HDR|DATA. All subsequent buffers are DATA only
   // virtq passes us the pointer to the DATA so we need to adjust
   // the number fo copied data and the pointer
   var adjust = 0;
   if( this.tx.tx_mrg_hdr[0].num_buffers != 0 ) {
      adjust = virtio_net_hdr_mrg_rxbuf_size;
   }

   // calculate the amont of data to copy on this pass
   // take the minimum of the datat left in the packet
   // and the adjusted buffer len
   var to_copy = math.min(tx_p.length - this.tx.data_sent, len + adjust);

   // copy the data to the adjusted pointer
   ffi.copy(pointer - adjust, tx_p.data + this.tx.data_sent, to_copy);

   // update the num_buffers in the first virtio header
   this.tx.tx_mrg_hdr[0].num_buffers = this.tx.tx_mrg_hdr[0].num_buffers + 1;
   this.tx.data_sent = this.tx.data_sent + to_copy;

   // have we sent all the data in the packet?
   if( this.tx.data_sent == tx_p.length ) {
      this.tx.finished = true;
   }

   // XXX The "adjust" is needed to counter-balance an adjustment made
   // in virtq_device. If we don't make this adjustment then we break
   // chaining together multiple buffers in that we report the size of
   // each buffer (except for the first) to be 12 bytes more than it
   // really is. This causes the VM to see an inflated ethernet packet
   // size which may or may not be noticed by an application.
   //
   // This formulation is not optimal and it would be nice to make
   // this code more transparent. -luke
   return to_copy - adjust;
}

function VirtioNetDevice::tx_packet_end_mrg_rxbuf(header_id, total_size, tx_p) {
   // free the packet only when all its data is processed
   if( this.tx.finished ) {
      packet.free(tx_p);
      this.tx.p = null;
      this.tx.data_sent = null;
      this.tx.finished = null;
   } else if( ! this.tx.p ) {
      this.tx.p = tx_p;
   }
   this.virtq[this.ring_id]->put_buffer(header_id, total_size);
}

// Advance the rx used ring and signal up
function VirtioNetDevice::tx_signal_used() {
   for( i = 0, this.virtq_pairs-1 ) {
      this.virtq[2*i]->signal_used();
   }
}

function VirtioNetDevice::map_from_guest (addr) {
   var result;
   var m = this.mem_table[0];
   // Check cache first (on-trace fastpath)
   if( addr >= m.guest && addr < m.guest + m.size ) {
      return addr + m.snabb - m.guest;
   }
   // Looping case
   for( i = 0, table.getn(this.mem_table) ) {
      m = this.mem_table[i];
      if( addr >= m.guest && addr < m.guest + m.size ) {
         if( i != 0 ) {
            this.mem_table[i] = this.mem_table[0];
            this.mem_table[0] = m;
         }
         return addr + m.snabb - m.guest;
      }
   }
   error("mapping to host address failed" .. tostring(ffi.cast("void*",addr)));
}

function VirtioNetDevice::map_from_qemu (addr) {
   var result = null;
   for( i = 0, table.getn(this.mem_table) ) {
      var m = this.mem_table[i];
      if( addr >= m.qemu && addr < m.qemu + m.size ) {
         result = addr + m.snabb - m.qemu;
         break;
      }
   }
   if( ! result ) {
      error("mapping to host address failed" .. tostring(ffi.cast("void*",addr)));
   }
   return result;
}

function VirtioNetDevice::get_features() {
   print(string.format("Get features 0x%x\n%s",
                        tonumber(this.supported_features), 
                        get_feature_names(this.supported_features)));
   return this.supported_features;
}

function VirtioNetDevice::set_features(features) {
   print(string.format("Set features 0x%x\n%s", tonumber(features), get_feature_names(features)));
   this.features = features;
   if( band(this.features, C.VIRTIO_NET_F_MRG_RXBUF) == C.VIRTIO_NET_F_MRG_RXBUF ) {
      this.hdr_type = virtio_net_hdr_mrg_rxbuf_type;
      this.hdr_size = virtio_net_hdr_mrg_rxbuf_size;
      this.mrg_rxbuf = true;
   } else {
      this.hdr_type = virtio_net_hdr_type;
      this.hdr_size = virtio_net_hdr_size;
      this.mrg_rxbuf = false;
   }
   if( band(this.features, C.VIRTIO_RING_F_INDIRECT_DESC) == C.VIRTIO_RING_F_INDIRECT_DESC ) {
      for( i = 0, max_virtq_pairs-1 ) {
         // TXQ
         this.virtq[2*i]->enable_indirect_descriptors();
         // RXQ
         this.virtq[2*i+1]->enable_indirect_descriptors();
      }
   }
}

function VirtioNetDevice::set_vring_num(idx, num) {
   var n = tonumber(num);
   if( band(n, n - 1) != 0 ) {
      error("vring_num should be power of 2");
   }

   this.virtq[idx].vring_num = n;
   // update the curent virtq pairs
   this.virtq_pairs = math.max(this.virtq_pairs, math.floor(idx/2)+1);
}

function VirtioNetDevice::set_vring_call(idx, fd) {
   this.virtq[idx].callfd = fd;
}

function VirtioNetDevice::set_vring_kick(idx, fd) {
   this.virtq[idx].kickfd = fd;
}

function VirtioNetDevice::set_vring_addr(idx, ring) {

   this.virtq[idx].virtq = ring;
   this.virtq[idx].avail = tonumber(ring.used.idx);
   this.virtq[idx].used = tonumber(ring.used.idx);
   print(string.format("rxavail = %d rxused = %d", this.virtq[idx].avail, this.virtq[idx].used));
   ring.used.flags = C.VRING_F_NO_NOTIFY;
}

function VirtioNetDevice::ready() {
   return this.virtq[0].virtq && this.virtq[1].virtq;
}

function VirtioNetDevice::set_vring_base(idx, num) {
   this.virtq[idx].avail = num;
}

function VirtioNetDevice::get_vring_base(idx) {
   return this.virtq[idx].avail;
}

function VirtioNetDevice::set_mem_table(mem_table) {
   this.mem_table = mem_table;
}

function VirtioNetDevice::report() {
   debug("txavail", this.virtq[0].virtq.avail.idx,
      "txused", this.virtq[0].virtq.used.idx,
      "rxavail", this.virtq[1].virtq.avail.idx,
      "rxused", this.virtq[1].virtq.used.idx);
}

function VirtioNetDevice::rx_buffers() {
   return this.vring_transmit_buffers;
}

feature_names = {
   [C.VIRTIO_F_NOTIFY_ON_EMPTY]                 = "VIRTIO_F_NOTIFY_ON_EMPTY",
   [C.VIRTIO_RING_F_INDIRECT_DESC]              = "VIRTIO_RING_F_INDIRECT_DESC",
   [C.VIRTIO_RING_F_EVENT_IDX]                  = "VIRTIO_RING_F_EVENT_IDX",

   [C.VIRTIO_F_ANY_LAYOUT]                      = "VIRTIO_F_ANY_LAYOUT",
   [C.VIRTIO_NET_F_CSUM]                        = "VIRTIO_NET_F_CSUM",
   [C.VIRTIO_NET_F_GUEST_CSUM]                  = "VIRTIO_NET_F_GUEST_CSUM",
   [C.VIRTIO_NET_F_GSO]                         = "VIRTIO_NET_F_GSO",
   [C.VIRTIO_NET_F_GUEST_TSO4]                  = "VIRTIO_NET_F_GUEST_TSO4",
   [C.VIRTIO_NET_F_GUEST_TSO6]                  = "VIRTIO_NET_F_GUEST_TSO6",
   [C.VIRTIO_NET_F_GUEST_ECN]                   = "VIRTIO_NET_F_GUEST_ECN",
   [C.VIRTIO_NET_F_GUEST_UFO]                   = "VIRTIO_NET_F_GUEST_UFO",
   [C.VIRTIO_NET_F_HOST_TSO4]                   = "VIRTIO_NET_F_HOST_TSO4",
   [C.VIRTIO_NET_F_HOST_TSO6]                   = "VIRTIO_NET_F_HOST_TSO6",
   [C.VIRTIO_NET_F_HOST_ECN]                    = "VIRTIO_NET_F_HOST_ECN",
   [C.VIRTIO_NET_F_HOST_UFO]                    = "VIRTIO_NET_F_HOST_UFO",
   [C.VIRTIO_NET_F_MRG_RXBUF]                   = "VIRTIO_NET_F_MRG_RXBUF",
   [C.VIRTIO_NET_F_STATUS]                      = "VIRTIO_NET_F_STATUS",
   [C.VIRTIO_NET_F_CTRL_VQ]                     = "VIRTIO_NET_F_CTRL_VQ",
   [C.VIRTIO_NET_F_CTRL_RX]                     = "VIRTIO_NET_F_CTRL_RX",
   [C.VIRTIO_NET_F_CTRL_VLAN]                   = "VIRTIO_NET_F_CTRL_VLAN",
   [C.VIRTIO_NET_F_CTRL_RX_EXTRA]               = "VIRTIO_NET_F_CTRL_RX_EXTRA",
   [C.VIRTIO_NET_F_CTRL_MAC_ADDR]               = "VIRTIO_NET_F_CTRL_MAC_ADDR",
   [C.VIRTIO_NET_F_CTRL_GUEST_OFFLOADS]         = "VIRTIO_NET_F_CTRL_GUEST_OFFLOADS",

   [C.VIRTIO_NET_F_MQ]                          = "VIRTIO_NET_F_MQ",

   [C.VHOST_USER_F_PROTOCOL_FEATURES]           = "VHOST_USER_F_PROTOCOL_FEATURES"
};

// Request fresh Just-In-Time compilation of the vring processing code.
// 
// This should be called when the expected workload has changed
// significantly, for example when a virtual machine loads a new
// device driver or renegotiates features. This will cause LuaJIT to
// generate fresh machine code for the traffic processing fast-path.
//
// See background motivation here:
//   https://github.com/LuaJIT/LuaJIT/issues/208#issuecomment-236423732
function VirtioNetDevice::rejit () {
   var mod = "lib.virtio.virtq_device";
   // Load fresh copies of the virtq module: one for tx, one for rx.
   var txvirtq = package.loaders[1](mod)(mod);
   var rxvirtq = package.loaders[1](mod)(mod);
   var tx_mt = {__index = txvirtq};
   var rx_mt = {__index = rxvirtq};
   for( i = 0, max_virtq_pairs-1 ) {
      setmetatable(this.virtq[2*i],   tx_mt);
      setmetatable(this.virtq[2*i+1], rx_mt);
   }
}

function get_feature_names(bits) {
var string = "";
   for( mask,name in pairs(feature_names) ) {
      if( (bit.band(bits,mask) == mask) ) {
         string = string .. " " .. name;
      }
   }
   return string;
}

function debug (...) {
   if( _G.developer_debug ) { print(...); }
}
